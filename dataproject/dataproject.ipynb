{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainy days on the stock market"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataproject by Josefine Pedersen, Viktor Texel and Pernille Svendsen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Table of contents** \n",
    "> - Import and set magics\n",
    "> - Introduction\n",
    "> - Read and clean data from DMI and Yahoo Finance\n",
    "> - Explore each dataset\n",
    "> - Merge datasets\n",
    "> - Analysis\n",
    "> - Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Imports and set magics:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import requests # library for making HTTP requests\n",
    "import datetime as dt # library for handling date and time objects\n",
    "\n",
    "\n",
    "# autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# user written modules\n",
    "import dataproject\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this dataproject we wish to explore if there could be a correlation between weather and developments on the stock market. Through API's we import datasets from DMI and Yahoo Finance to examine whether or not there is a correlation between price fluktuations in the danish OMX C25-index and the amount of precipitation that falls in Denmark. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and clean data from DMI and Yahoo Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your data, either through an API or manually, and load it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import data from DMI**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install a package to inspect data from DMI (Danish Meteorological Institute):\n",
    "\n",
    "#%pip install dmi-open-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our API-key given to us from DMI's database\n",
    "api_key = 'bd463c7d-f6f8-431d-a5a7-c466766a8363'\n",
    "\n",
    "DMI_URL = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "r = requests.get(DMI_URL, params={'api-key': api_key}) # Issues a HTTP GET request\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = r.json()  # Extract JSON data\n",
    "print(json.keys())  # Print the keys of the JSON dictionary\n",
    "\n",
    "df = pd.json_normalize(json['features'])  # Convert JSON object to a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['properties.observed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_ids = df['properties.parameterId'].unique()  # Generate a list of unique parameter ids\n",
    "print(parameter_ids)  # Print all unique parameter ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired start and end time\n",
    "start_time = pd.Timestamp(2022, 1, 1)\n",
    "end_time = pd.Timestamp(2023, 1, 1)\n",
    "\n",
    "# Specify one or more station IDs or all_stations\n",
    "all_stationsDK = [\n",
    "    '05005', '05009', '05015', '05031', '05035', '05042', '05065', \n",
    "    '05070', '05075', '05081', '05085', '05089', '05095', '05105', \n",
    "    '05109', '05135', '05140', '05150', '05160', '05165', '05169', \n",
    "    '05185', '05199', '05202', '05205', '05220', '05225', '05269', \n",
    "    '05272', '05276', '05277', '05290', '05296', '05300', '05305', \n",
    "    '05320', '05329', '05343', '05345', '05350', '05355', '05365', \n",
    "    '05375', '05381', '05395', '05400', '05406', '05408', '05435', \n",
    "    '05440', '05450', '05455', '05469', '05499', '05505', '05510', \n",
    "    '05529', '05537', '05545', '05575', '05735', '05880', '05889', \n",
    "    '05935', '05945', '05970', '05986', '05994'\n",
    "]\n",
    "\n",
    "# Specify one or more parameter IDs or all_parameters\n",
    "parameterId = ['precip_past1h']\n",
    "\n",
    "# Derive datetime specifier string\n",
    "datetime_str = start_time.tz_localize('UTC').isoformat() + '/' + end_time.tz_localize('UTC').isoformat()\n",
    "\n",
    "dfs = []\n",
    "for station in all_stationsDK:\n",
    "    for parameter in parameterId:\n",
    "        # Specify query parameters\n",
    "        params = {\n",
    "            'api-key' : api_key,\n",
    "            'datetime' : datetime_str,\n",
    "            'stationId' : station,\n",
    "            'parameterId' : parameter,\n",
    "            'limit' : '300000',  # max limit\n",
    "        }\n",
    "\n",
    "        # Submit GET request with url and parameters\n",
    "        r = requests.get(DMI_URL, params=params)\n",
    "        # Extract JSON object\n",
    "        json = r.json() # Extract JSON object\n",
    "        # Convert JSON object to a MultiIndex DataFrame and add to list\n",
    "        dfi = pd.json_normalize(json['features'])\n",
    "        if dfi.empty is False:\n",
    "            dfi['time'] = pd.to_datetime(dfi['properties.observed'])\n",
    "            dfi[['station', 'parameter']] = station, parameter\n",
    "            dfi = dfi.set_index(['parameter', 'station', 'time'])\n",
    "            dfi = dfi['properties.value'].unstack(['station','parameter'])\n",
    "            dfs.append(dfi)\n",
    "\n",
    "df = pd.concat(dfs, axis='columns').sort_index()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type('time'))\n",
    "\n",
    "df.reset_index(inplace=True) \n",
    "list(df.columns)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a row-average of the observations across weather stations\n",
    "df['mean'] = df.mean(axis=1)\n",
    "df['day'] = df.time.dt.date \n",
    "df2 = df[['time', 'day', 'mean']] \n",
    "df2.drop(df2.tail(1).index,inplace=True) # drop last n rows\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check to see which days have missing hours\n",
    "tjek = df2.groupby(['day']).count()\n",
    "tjek\n",
    "\n",
    "tjek2 = tjek.loc[tjek['time']!=24]\n",
    "tjek2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. create the figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# b. plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.plot(df2['day'],df2['mean'])\n",
    "\n",
    "ax.set_title('Value of choice, $u(x_1,1)$')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$u(x_1,1)$');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import data from Yahoo Finance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "df = web.DataReader('GE', 'yahoo', start='2019-09-10', end='2019-10-09')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore each data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to **explore the raw data**, you may provide **static** and **interactive plots** to show important developments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive plot** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func():\n",
    "    # Function that operates on data set\n",
    "    pass\n",
    "\n",
    "widgets.interact(plot_func, \n",
    "    # Let the widget interact with data through plot_func()    \n",
    "); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what you see when moving elements of the interactive plot around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you create combinations of your loaded data sets. Remember the illustration of a (inner) **merge**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "v = venn2(subsets = (4, 4, 10), set_labels = ('Data X', 'Data Y'))\n",
    "v.get_label_by_id('100').set_text('dropped')\n",
    "v.get_label_by_id('010').set_text('dropped' )\n",
    "v.get_label_by_id('110').set_text('included')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are dropping elements from both data set X and data set Y. A left join would keep all observations in data X intact and subset only from Y. \n",
    "\n",
    "Make sure that your resulting data sets have the correct number of rows and columns. That is, be clear about which observations are thrown away. \n",
    "\n",
    "**Note:** Don't make Venn diagrams in your own data project. It is just for exposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick overview of the data, we show some **summary statistics** on a meaningful aggregation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE FURTHER ANALYSIS. EXPLAIN THE CODE BRIEFLY AND SUMMARIZE THE RESULTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD CONCISE CONLUSION."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "47ef90cdf3004d3f859f1fb202523c65c07ba7c22eefd261b181f4744e2d0403"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
