{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainy days on the stock market"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataproject by Josefine Pedersen, Viktor Texel and Pernille Svendsen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Table of contents** \n",
    "> - Import and set magics\n",
    "> - Introduction\n",
    "- Indsæt billede og forklar data\n",
    "> - Read and clean data from DMI and Yahoo Finance\n",
    "> - Explore each dataset\n",
    "- 2 plots med hhv. nedbør og OMXC25 fordelt på måneder --> 1 plot med 2 firkanter heri hvor hvert plot kan ses. Herudover også et interaktivt plot hvor man kan udvælge på specifikke måneder for begge plots samtidig.  \n",
    "> - Merge datasets\n",
    "> - Analysis\n",
    "- Vi skal flytte change_stock til analysis og vise de 10 dage med mest nedgang/fremgang på stockmarkedet og sammenholde med nedbør på disse dage. \n",
    "> - Conclusion\n",
    "\n",
    "- Overvej mulighed for at streamline data ved at lægge ind i py-filen. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Imports and set magics:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import requests # library for making HTTP requests\n",
    "import datetime as dt # library for handling date and time objects\n",
    "\n",
    "\n",
    "# autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# user written modules\n",
    "import dataproject\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this dataproject we wish to explore if there could be a correlation between weather and developments on the stock market. Through API's we import datasets from DMI and Yahoo Finance to examine whether or not there is a correlation between price fluktuations in the danish OMX C25-index and the amount of precipitation that falls in Denmark. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and clean data from DMI and Yahoo Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your data, either through an API or manually, and load it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import data from DMI**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install a package to inspect data from DMI (Danish Meteorological Institute):\n",
    "\n",
    "#%pip install dmi-open-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our API-key given to us from DMI's database\n",
    "api_key = 'bd463c7d-f6f8-431d-a5a7-c466766a8363'\n",
    "\n",
    "DMI_URL = 'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items'\n",
    "r = requests.get(DMI_URL, params={'api-key': api_key}) # Issues a HTTP GET request\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = r.json()  # Extract JSON data\n",
    "print(json.keys())  # Print the keys of the JSON dictionary\n",
    "\n",
    "df = pd.json_normalize(json['features'])  # Convert JSON object to a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['properties.observed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_ids = df['properties.parameterId'].unique()  # Generate a list of unique parameter ids\n",
    "print(parameter_ids)  # Print all unique parameter ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired start and end time\n",
    "start_time = pd.Timestamp(2022, 1, 1)\n",
    "end_time = pd.Timestamp(2023, 1, 1)\n",
    "\n",
    "# Specify one or more station IDs or all_stations\n",
    "all_stationsDK = [\n",
    "    '05005', '05009', '05015', '05031', '05035', '05042', '05065', \n",
    "    '05070', '05075', '05081', '05085', '05089', '05095', '05105', \n",
    "    '05109', '05135', '05140', '05150', '05160', '05165', '05169', \n",
    "    '05185', '05199', '05202', '05205', '05220', '05225', '05269', \n",
    "    '05272', '05276', '05277', '05290', '05296', '05300', '05305', \n",
    "    '05320', '05329', '05343', '05345', '05350', '05355', '05365', \n",
    "    '05375', '05381', '05395', '05400', '05406', '05408', '05435', \n",
    "    '05440', '05450', '05455', '05469', '05499', '05505', '05510', \n",
    "    '05529', '05537', '05545', '05575', '05735', '05880', '05889', \n",
    "    '05935', '05945', '05970', '05986', '05994'\n",
    "]\n",
    "\n",
    "# Specify one or more parameter IDs or all_parameters\n",
    "parameterId = ['precip_past1h']\n",
    "\n",
    "# Derive datetime specifier string\n",
    "datetime_str = start_time.tz_localize('UTC').isoformat() + '/' + end_time.tz_localize('UTC').isoformat()\n",
    "\n",
    "dfs = []\n",
    "for station in all_stationsDK:\n",
    "    for parameter in parameterId:\n",
    "        # Specify query parameters\n",
    "        params = {\n",
    "            'api-key' : api_key,\n",
    "            'datetime' : datetime_str,\n",
    "            'stationId' : station,\n",
    "            'parameterId' : parameter,\n",
    "            'limit' : '300000',  # max limit\n",
    "        }\n",
    "\n",
    "        # Submit GET request with url and parameters\n",
    "        r = requests.get(DMI_URL, params=params)\n",
    "        # Extract JSON object\n",
    "        json = r.json() # Extract JSON object\n",
    "        # Convert JSON object to a MultiIndex DataFrame and add to list\n",
    "        dfi = pd.json_normalize(json['features'])\n",
    "        if dfi.empty is False:\n",
    "            dfi['Time'] = pd.to_datetime(dfi['properties.observed'])\n",
    "            dfi[['station', 'parameter']] = station, parameter\n",
    "            #dfi = dfi.set_index(['parameter', 'station', 'Time'])\n",
    "            #dfi = dfi['properties.value'].unstack(['station','parameter'])\n",
    "            dfi = dfi.set_index(['station', 'Time'])\n",
    "            dfi = dfi['properties.value'].unstack(['station'])\n",
    "            dfs.append(dfi)\n",
    "\n",
    "df = pd.concat(dfs, axis='columns').sort_index()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True) \n",
    "list(df.columns)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a row-average of the observations across weather stations\n",
    "df['Precip'] = df.mean(axis=1)\n",
    "df['Date'] = df.Time.dt.date \n",
    "df2 = df[['Time', 'Date', 'Precip']] \n",
    "df2.drop(df2.tail(1).index,inplace=True) # drop last n rows\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check to see which days have missing hours\n",
    "tjek = df2.groupby(['Date'])['Time'].count()\n",
    "tjek = pd.DataFrame(tjek)\n",
    "tjek\n",
    "\n",
    "tjek2 = tjek.loc[tjek['Time']!=24]\n",
    "tjek2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupby('Date').mean()\n",
    "df3.reset_index(inplace=True) \n",
    "df3['Date'] =pd.to_datetime(df3['Date'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. create the figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# b. plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.bar(df3['Date'],df3['Precip'])\n",
    "\n",
    "ax.set_title('Average precipation in 2022')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Precipation');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import data from Yahoo Finance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install nescesarry packages for import\n",
    "\n",
    "#%pip install yfinance\n",
    "#%pip install yahoofinancials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "from yahoofinancials import YahooFinancials\n",
    "\n",
    "OMXC25 = yf.download('^OMXC25', start='2022-01-01', end='2023-01-01', progress=False)\n",
    "OMXC25.reset_index(inplace=True) \n",
    "OMXC25['Date'] =  pd.to_datetime(OMXC25['Date'])\n",
    "OMXC25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore each data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to **explore the raw data**, you may provide **static** and **interactive plots** to show important developments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive plot** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func():\n",
    "    # Function that operates on data set\n",
    "    pass\n",
    "\n",
    "widgets.interact(plot_func, \n",
    "    # Let the widget interact with data through plot_func()    \n",
    "); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what you see when moving elements of the interactive plot around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create combinations of our loaded data sets from DMI and Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_stock = pd.merge(OMXC25, df3, on='Date', how='left')\n",
    "precip_stock2 = precip_stock[['Date', 'Close', 'Precip']]\n",
    "precip_stock2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the pct. change from day-to-day on the closing price of OMXC25\n",
    "precip_stock2['Change_in_stock'] = ((precip_stock2['Close'] / precip_stock2['Close'].shift(1) - 1)* 100)\n",
    "precip_stock2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at monthly data to get a view of trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_stock3 = precip_stock2\n",
    "precip_stock3['Month'] = precip_stock3.Date.dt.month \n",
    "precip_stock3 = precip_stock3.groupby('Month').mean()\n",
    "precip_stock3.reset_index(inplace=True) \n",
    "precip_stock3\n",
    "\n",
    "#trend = precip_stock2.groupby(['Date']).count()\n",
    "#trend\n",
    "\n",
    "#tjek2 = tjek.loc[tjek['Time']!=24]\n",
    "#tjek2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are dropping elements from both data set X and data set Y. A left join would keep all observations in data X intact and subset only from Y. \n",
    "\n",
    "Make sure that your resulting data sets have the correct number of rows and columns. That is, be clear about which observations are thrown away. \n",
    "\n",
    "**Note:** Don't make Venn diagrams in your own data project. It is just for exposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick overview of the data, we show some **summary statistics** on a meaningful aggregation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE FURTHER ANALYSIS. EXPLAIN THE CODE BRIEFLY AND SUMMARIZE THE RESULTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD CONCISE CONLUSION."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "47ef90cdf3004d3f859f1fb202523c65c07ba7c22eefd261b181f4744e2d0403"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
